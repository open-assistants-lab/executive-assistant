# Executive Assistant Technical Architecture Documentation

**Version:** 1.3.0
**Last Updated:** February 1, 2026
**Project:** Executive Assistant - Multi-channel AI Agent Platform

**Recent Updates (February 2026):**
- âœ… **Implemented User MCP Management** - Per-conversation MCP server management
  - User-managed MCP servers (stdio + HTTP/SSE)
  - Tiered tool loading (user > admin priority)
  - Tool deduplication and hot-reload with `clear_mcp_cache()`
  - Automatic backup/restore with rotation (keeps last 5)
  - Security validation (HTTPS enforcement, server name validation, command injection prevention)
  - Storage: `data/users/{thread_id}/mcp/mcp.json` and `mcp_remote.json`
- âœ… **Implemented MCP-Skill HITL Integration** - Human-in-the-loop skill loading
  - Auto-detection of relevant skills when adding MCP servers
  - Pending skill proposals with approval workflow
  - Skill mapping database (fetch, github, clickhouse, filesystem, brave-search, puppeteer)
  - 5 HITL workflow tools: `mcp_list_pending_skills`, `mcp_approve_skill`, `mcp_reject_skill`, `mcp_edit_skill`, `mcp_show_skill`
  - Enhanced `mcp_add_server` creates proposals, `mcp_reload` loads approved skills
  - Storage: `data/users/{thread_id}/mcp/pending_skills/{skill_name}.json`
  - 60 comprehensive tests (33 storage/mapping + 27 workflow tools)
  - Files: `storage/mcp_skill_storage.py`, `tools/mcp_skill_mapping.py`, enhanced `tools/user_mcp_tools.py`

**Previous Updates (January 2026):**
- âœ… **Implemented Instinct System** - Automatic behavioral pattern learning
  - Observer: Pattern detection (corrections, repetitions, preferences)
  - Injector: Context injection into system prompts
  - Evolver: Clustering instincts into skills
  - Profiles: 6 pre-built personality presets
- âœ… Migrated to LangChain agent runtime (removed custom nodes.py)
- âœ… Implemented token usage tracking for HTTP channel (OpenAI/Anthropic)
- âœ… Added comprehensive middleware stack (summarization, retry, status updates)
- âœ… Fixed progressive disclosure bug (all 87 tools now available by default)
- âœ… Added ThreadContextMiddleware for async context propagation
- âœ… Enhanced error logging with comprehensive tracebacks
- âœ… Fixed HTTP channel non-streaming endpoint
- âœ… HTTP channel now bypasses allowlist (frontend auth pattern)

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Technology Stack](#technology-stack)
3. [Architecture Overview](#architecture-overview)
4. [Code Structure](#code-structure)
5. [Core Components](#core-components)
6. [Data Flow](#data-flow)
7. [Storage Architecture](#storage-architecture)
8. [Key Libraries & Frameworks](#key-libraries--frameworks)
9. [Configuration Management](#configuration-management)
10. [Deployment Architecture](#deployment-architecture)
11. [Testing Strategy](#testing-strategy)

---

## Project Overview

Executive Assistant is a **multi-channel AI agent platform** built on LangGraph that implements a ReAct (Reasoning + Acting) agent pattern. It provides intelligent task execution across multiple communication channels (Telegram, HTTP) with persistent state management, privacy-first multi-tenant storage, and a comprehensive toolkit for data operations.

**Key Characteristics:**
- **LangChain Agent Runtime**: High-level agent creation with middleware stack
- **ReAct Agent Pattern**: Reasoning â†’ Action â†’ Observation cycle
- **Multi-Channel Support**: Telegram bot and HTTP REST API with SSE streaming
- **Privacy-First Storage**: Thread-only context with per-thread data isolation
- **Tool-Based Intelligence**: All 87 tools available in every conversation
- **Instincts System**: Automatic behavioral pattern learning with confidence scoring
- **State Persistence**: PostgreSQL-backed checkpointing for conversation memory
- **Token Tracking**: Automatic usage monitoring for cost control (provider-dependent)
- **Production-Ready**: Middleware stack with summarization, retry logic, status updates, and call limits

---

## Technology Stack

### Core Frameworks
| Category | Technology | Purpose |
|----------|-----------|---------|
| **Orchestration** | LangGraph v1.0.6+ | Agent workflow/state management |
| **LLM Abstraction** | LangChain v0.3.27+ | Unified LLM interface |
| **LLM Runtime** | LangGraph-Prebuilt v1.0.6+ | Prebuilt agent components |
| **HTTP Server** | FastAPI v0.115.0+ | REST API with streaming |
| **Async Runtime** | uvicorn | ASGI server |

### LLM Providers
- **Anthropic**: Claude models (Claude Haiku/Sonnet)
- **OpenAI**: GPT models (GPT-4o, GPT-4o-mini)
- **Zhipu AI**: GLM-4 models
- **Ollama**: Local/Cloud OpenAI-compatible models

### Data Storage
| Type | Technology | Purpose |
|------|-----------|---------|
| **State/Checkpoint** | PostgreSQL (via asyncpg) | Conversation persistence |
| **Vector Database** | LanceDB | Semantic search/knowledge base |
| **Tabular Data** | SQLite (sqlite_db_storage.py, tdb_tools.py) | Transactional, permanent data (timesheets, CRM, tasks) |
| **Memories** | SQLite + FTS5 (mem_storage.py) | Embedded memories with full-text search |
| **File Storage** | Local filesystem | Document/file storage |
| **Metadata Registry** | PostgreSQL | File/DB ownership tracking |

### MCP Configuration Storage
| Type | Location | Purpose |
|------|----------|---------|
| **Admin MCP** | `data/admins/mcp.json` | Admin-supplied MCP servers (applies globally) |

### Supporting Libraries
- **Job Scheduling**: APScheduler v3.10.0+
- **Task Queue**: temporalio v1.21.1+ (optional)
- **OCR**: Surya-OCR / PaddleOCR (optional, Linux x86_64)
- **Data Processing**: Pandas v2.2.0+, PyArrow
- **HTTP Client**: httpx
- **Logging**: loguru
- **Configuration**: Pydantic v2.12.5+, PyYAML

### Development Tools
- **Testing**: pytest v9.0.2+, pytest-asyncio
- **Package Management**: uv (Python 3.11+)
- **Containerization**: Docker, docker-compose

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Channels Layer                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   TelegramChannel      â”‚         HttpChannel                    â”‚
â”‚  (python-telegram-bot)â”‚       (FastAPI + SSE)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Middleware Stack    â”‚
                â”‚ â€¢ Summarization      â”‚
                â”‚ â€¢ Call Limits        â”‚
                â”‚ â€¢ Retry Logic        â”‚
                â”‚ â€¢ Todo Tracking      â”‚
                â”‚ â€¢ Status Updates     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   LangGraph ReAct      â”‚
                â”‚   Agent Graph          â”‚
                â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                â”‚ â”‚    call_model      â”‚ â”‚
                â”‚ â”‚    (LLM Reasoning) â”‚ â”‚
                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                â”‚           â”‚             â”‚
                â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                â”‚ â”‚   call_tools      â”‚ â”‚
                â”‚ â”‚ (Tool Execution)  â”‚ â”‚
                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                 â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚   File    â”‚   â”‚ Database    â”‚   â”‚   Vector    â”‚
    â”‚  Sandbox  â”‚   â”‚  Tools      â”‚   â”‚   Store     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Storage Backends     â”‚
                â”‚ â€¢ File System         â”‚
                â”‚ â€¢ SQLite             â”‚
                â”‚ â€¢ LanceDB             â”‚
                â”‚ â€¢ PostgreSQL          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ReAct Agent Flow

The agent follows the **ReAct (Reasoning + Acting)** pattern:

1. **Reason**: LLM analyzes user request and decides on actions
2. **Act**: Execute tools (file operations, database queries, web search, etc.)
3. **Observe**: Process tool results and update context
4. **Loop**: Repeat until task complete or iteration limit reached

**State Transition:**
```
[Start] â†’ [call_model] â†’ [has tool_calls?] â”€â”€â”€â”€Noâ”€â”€â†’ [END]
                    â”‚                             Yes
                    â†“
               [call_tools] â†’ [increment_iterations]
                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [call_model]
```

---

## Code Structure

```
executive_assistant/
â”œâ”€â”€ src/executive_assistant/                    # Main application package
â”‚   â”œâ”€â”€ agent/                     # Agent logic & graph
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph StateGraph definition
â”‚   â”‚   â”œâ”€â”€ nodes.py              # ReAct nodes: call_model, call_tools
â”‚   â”‚   â”œâ”€â”€ state.py              # AgentState TypedDict
â”‚   â”‚   â”œâ”€â”€ prompts.py            # System prompts for reasoning
â”‚   â”‚   â”œâ”€â”€ langchain_agent.py    # LangChain agent runtime
â”‚   â”‚   â”œâ”€â”€ status_middleware.py   # Real-time progress tracking
â”‚   â”‚   â”œâ”€â”€ middleware_debug.py    # Debug middleware
â”‚   â”‚   â”œâ”€â”€ todo_display.py       # Todo list display logic
â”‚   â”‚   â”œâ”€â”€ topic_classifier.py   # Message classification
â”‚   â”‚   â”œâ”€â”€ router.py             # Conditional edge routing
â”‚   â”‚   â”œâ”€â”€ checkpoint_utils.py   # Checkpoint management
â”‚   â”‚   â””â”€â”€ langchain_state.py    # LangChain state wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ channels/                  # Communication channels
â”‚   â”‚   â”œâ”€â”€ base.py              # Abstract BaseChannel class
â”‚   â”‚   â”œâ”€â”€ telegram.py          # Telegram bot implementation
â”‚   â”‚   â”œâ”€â”€ http.py              # FastAPI HTTP channel
â”‚   â”‚   â””â”€â”€ management_commands.py # CLI commands (/mem, /vdb, /tdb, /file)
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                   # Data persistence layer
â”‚   â”‚   â”œâ”€â”€ checkpoint.py        # LangGraph PostgreSQL checkpointer
â”‚   â”‚   â”œâ”€â”€ file_sandbox.py      # Secure file operations (thread-scoped)
â”‚   â”‚   â”œâ”€â”€ db_storage.py        # Legacy DuckDB TDB storage (deprecated)
â”‚   â”‚   â”œâ”€â”€ tdb_tools.py         # SQLite TDB tool implementations
â”‚   â”‚   â”œâ”€â”€ sqlite_db_storage.py # SQLite backend (context + shared)
â”‚   â”‚   â”œâ”€â”€ vdb_tools.py         # Vector database tool implementations
â”‚   â”‚   â”œâ”€â”€ lancedb_storage.py   # LanceDB vector database backend
â”‚   â”‚   â”œâ”€â”€ user_registry.py     # Conversation logs & ownership tracking
â”‚   â”‚   â”œâ”€â”€ meta_registry.py     # Metadata/ownership tracking
â”‚   â”‚   â”œâ”€â”€ reminder.py          # Reminder scheduling
â”‚   â”‚   â”œâ”€â”€ scheduled_flows.py    # APScheduler integration
â”‚   â”‚   â”œâ”€â”€ chunking.py         # Document chunking for vector database
â”‚   â”‚   â”œâ”€â”€ mem_storage.py      # Embedded memory storage
â”‚   â”‚   â”œâ”€â”€ instinct_storage.py # Instinct behavioral patterns (JSONL + snapshot)
â”‚   â”‚   â””â”€â”€ workers.py          # Async worker pool
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                    # LangChain tool implementations
â”‚   â”‚   â”œâ”€â”€ registry.py          # Tool registry (get_all_tools)
â”‚   â”‚   â”œâ”€â”€ python_tool.py       # Python code execution (sandboxed)
â”‚   â”‚   â”œâ”€â”€ time_tool.py        # Timezone-aware time queries
â”‚   â”‚   â”œâ”€â”€ reminder_tools.py   # Reminder CRUD operations
â”‚   â”‚   â”œâ”€â”€ search_tool.py      # Web search (SearXNG)
â”‚   â”‚   â”œâ”€â”€ ocr_tool.py        # OCR image/PDF text extraction
â”‚   â”‚   â”œâ”€â”€ firecrawl_tool.py   # Firecrawl web scraping
â”‚   â”‚   â”œâ”€â”€ mem_tools.py       # Memory extraction tools
â”‚   â”‚   â”œâ”€â”€ meta_tools.py      # System metadata queries
â”‚   â”‚   â””â”€â”€ confirmation_tool.py # Large operation confirmation
â”‚   â”‚
â”‚   â”œâ”€â”€ skills/                   # Dynamic skill loading system
â”‚   â”‚   â”œâ”€â”€ registry.py         # Skill registry
â”‚   â”‚   â”œâ”€â”€ loader.py           # Skill loader
â”‚   â”‚   â”œâ”€â”€ builder.py          # Skill graph builder
â”‚   â”‚   â”œâ”€â”€ tool.py             # Skill tool wrapper
â”‚   â”‚   â””â”€â”€ content/            # Skill definitions directory
â”‚   â”‚
â”‚   â”œâ”€â”€ instincts/                # Behavioral pattern learning system
â”‚   â”‚   â”œâ”€â”€ observer.py         # Pattern detection from interactions
â”‚   â”‚   â”œâ”€â”€ injector.py         # Context injection into system prompts
â”‚   â”‚   â”œâ”€â”€ evolver.py          # Clustering instincts into skills
â”‚   â”‚   â””â”€â”€ profiles.py         # Pre-built personality presets
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ settings.py         # Pydantic Settings class
â”‚   â”‚   â”œâ”€â”€ llm_factory.py      # LLM model factory
â”‚   â”‚   â”œâ”€â”€ loader.py           # Config loader
â”‚   â”‚   â””â”€â”€ constants.py        # Application constants
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ scheduler.py             # APScheduler integration
â”‚   â”œâ”€â”€ logging.py               # Loguru logging configuration
â”‚   â”œâ”€â”€ dev_server.py            # LangGraph dev server entry point
â”‚   â””â”€â”€ src/executive_assistant/main.py   # Application entry point
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_agent.py            # Agent integration tests
â”‚   â”œâ”€â”€ test_file_sandbox.py     # File sandbox tests
â”‚   â”œâ”€â”€ test_db_storage.py       # DuckDB storage tests (legacy)
â”‚   â”œâ”€â”€ test_lancedb_vdb.py      # Vector database tests
â”‚   â”œâ”€â”€ test_python_tool.py      # Python execution tests
â”‚   â”œâ”€â”€ test_status_middleware.py # Middleware tests
â”‚   â”œâ”€â”€ test_scheduled_flows.py   # Scheduler tests
â”‚   â”œâ”€â”€ test_temporal_api.py     # Temporal integration tests
â”‚   â””â”€â”€ conftest.py             # Pytest fixtures
â”‚
â”œâ”€â”€ docker/migrations/            # PostgreSQL schema migrations
â”‚   â””â”€â”€ 000_init.sql             # Initial tables (thread-only)
â”‚
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â””â”€â”€ benchmark_results/        # Performance benchmark results
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ kb/                      # Knowledge base docs
â”‚   â”œâ”€â”€ langchain-skills/        # LangChain skills documentation
â”‚   â””â”€â”€ ollama/                  # Ollama configuration
â”‚
â”œâ”€â”€ data/                         # Application data
â”‚   â”œâ”€â”€ shared/                  # Organization-wide storage
â”‚   â””â”€â”€ users/                   # Thread-scoped storage
â”‚       â””â”€â”€ {thread_id}/
â”‚           â”œâ”€â”€ files/
â”‚           â”œâ”€â”€ tdb/
â”‚           â”œâ”€â”€ vdb/
â”‚           â””â”€â”€ mem/
â”‚
â”œâ”€â”€ pyproject.toml                # Project dependencies & scripts
â”œâ”€â”€ docker/config.yaml                   # Default configuration
â”œâ”€â”€ docker/.env.example                  # Environment template
â”œâ”€â”€ docker/Dockerfile                    # Container definition
â”œâ”€â”€ docker/docker-compose.yml            # Development stack
â”œâ”€â”€ langgraph.json                # LangGraph CLI configuration
â”œâ”€â”€ README.md                     # User documentation
â”œâ”€â”€ TODO.md                       # Development roadmap
â””â”€â”€ CLAUDE.md                    # Development workflow notes
```

---

## Core Components

### 1. Agent Layer (`src/executive_assistant/agent/`)

**Purpose:** Implements the ReAct reasoning loop and manages agent state.

**Key Files:**
- `graph.py`: Defines LangGraph StateGraph with ReAct nodes
- `nodes.py`: Core node implementations
  - `call_model()`: Invokes LLM with conversation history
  - `call_tools()`: Executes LangChain tools
  - `increment_iterations()`: Tracks reasoning cycles
- `state.py`: AgentState TypedDict containing:
  - `messages`: Conversation history (with `add_messages` reducer)
  - `iterations`: Loop counter (prevents infinite loops)
  - `user_id`: Thread identifier (channel + channel user id)
  - `channel`: Source channel (telegram/http)
  - `structured_summary`: Topic-based conversation summary
  - `todos`: Task tracking list
- `prompts.py`: System prompts for LLM reasoning
- `langchain_agent.py`: LangChain agent runtime builder with middleware stack
- `status_middleware.py`: Real-time progress tracking with millisecond timing
- `todo_display.py`: Todo list display formatting
- `token_callbacks.py`: Token usage tracking (experimental, unused)

**Middleware Stack (via LangChain):**
1. **StatusUpdateMiddleware**: Real-time progress tracking with emoji indicators (ðŸ¤” Thinking, ðŸ› ï¸ Tool N:, âœ… Done)
2. **ThreadContextMiddleware**: Ensures thread_id ContextVar propagates to tool execution
3. **TodoListMiddleware**: Tracks planned tasks for multi-step operations
4. **TodoListDisplayMiddleware**: Displays planned tasks during execution (if channel enabled)
5. **SummarizationMiddleware**: Token-based conversation summarization (trigger: 5,000 / target: 1,000)
6. **ContextEditingMiddleware**: Edits context by clearing old tool uses (disabled by default)
7. **ModelCallLimitMiddleware**: Max 50 LLM calls per message (prevents infinite loops)
8. **ToolCallLimitMiddleware**: Max 100 tool calls per message (prevents runaway execution)
9. **ToolRetryMiddleware**: Automatic retry on tool failures with exponential backoff
10. **ModelRetryMiddleware**: Automatic retry on model failures with exponential backoff

**ThreadContextMiddleware (Custom):**
- **Purpose**: Fix Python ContextVar not propagating across LangGraph async task boundaries
- **Implementation**: Wraps tool execution via `awrap_tool_call()`
- **Functionality**:
  - Captures current `thread_id` from ContextVar before tool call
  - Restores `thread_id` immediately before tool execution
  - Logs all tool errors with full traceback at DEBUG level
- **Critical for**: FileSandbox, TDB, VDB, and all thread-scoped storage operations

### 2. Channels Layer (`src/executive_assistant/channels/`)

**Purpose:** Handles communication between users and agent across different platforms.

#### BaseChannel (`base.py`)
Abstract base class defining channel interface:
- `start()`: Initialize channel (connect to platform)
- `stop()`: Graceful shutdown
- `send_message()`: Send response to user
- `update_status()`: Update in-progress status
- `display_todos()`: Show todo list

#### TelegramChannel (`telegram.py`)
- Uses `python-telegram-bot` v22.5+
- Bot Commands: `/start`, `/reset`, `/remember`, `/debug`, `/mem`, `/reminder`, `/vdb`, `/tdb`, `/file`, `/meta`, `/user`
- Features:
  - Inline message editing for status updates (clean UI)
  - Debug mode toggle for verbose timing information
  - Per-thread asyncio locks to prevent message interleaving
  - Message queuing with deduplication
- Thread ID: Uses Telegram `chat_id` as `thread_id`
- Thread-only context (no group support)

#### HttpChannel (`http.py`)
- Built with FastAPI v0.115.0+
- Endpoints:
  - `POST /message`: Send message (supports SSE streaming with `stream: true`)
  - `GET /conversations/{id}`: Get conversation history
  - `GET /health`: Health check
- Features:
  - Server-sent events (SSE) for real-time streaming
  - JSON request/response models (Pydantic)
  - **Open access** - authentication handled by frontend application
  - **Non-streaming endpoint** - collects all messages and returns as JSON array
- Thread ID: Format `http:{conversation_id}`
- Authorization: Bypasses allowlist (frontend auth pattern)

### 3. Storage Layer (`src/executive_assistant/storage/`)

**Purpose:** Provides thread-scoped, privacy-first data storage with multi-tenancy support.

#### Storage Hierarchy

```
data/
â”œâ”€â”€ admins/           # admin-managed configuration and allowlist
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ prompt.md
â”‚   â”œâ”€â”€ skills/
â”‚   â”œâ”€â”€ mcp.json
â”‚   â””â”€â”€ user_allowlist.json
â”œâ”€â”€ shared/           # scope="shared" (organization-wide)
â”‚   â”œâ”€â”€ files/
â”‚   â”œâ”€â”€ tdb/
â”‚   â””â”€â”€ vdb/
â””â”€â”€ users/            # scope="context" (thread-only)
    â””â”€â”€ {thread_id}/
        â”œâ”€â”€ files/
        â”œâ”€â”€ tdb/
        â”œâ”€â”€ vdb/
        â”œâ”€â”€ mem/
        â””â”€â”€ instincts/    # Learned behavioral patterns
            â”œâ”€â”€ instincts.jsonl
            â””â”€â”€ instincts.snapshot.json
```

#### Key Storage Components

**FileSandbox (`file_sandbox.py`)**
- **Purpose**: Secure file operations with thread isolation
- **Isolation**: Uses Python `ContextVar` for thread-scoped paths
- **Features**:
  - Path traversal protection (prevents `../../../` attacks)
  - File size limits (configurable via `MAX_FILE_SIZE_MB`)
  - File extension whitelisting
  - Ownership tracking in PostgreSQL
- **Tools**:
  - `read_file`, `write_file`: Text file I/O
  - `create_folder`, `delete_folder`, `rename_folder`: Directory management
  - `move_file`: File relocation
  - `glob_files`: Pattern matching (`*.py`, `**/*.json`)
  - `grep_files`: Regex content search

**TDBStorage (`sqlite_db_storage.py`, `tdb_tools.py`)**
- **Backend**: SQLite (context + shared)
- **Purpose**: Structured/tabular data storage
- **Use Cases**: Timesheets, logs, analysis datasets
- **Tools**:
  - `create_tdb_table`: Create from JSON/CSV with auto schema inference
  - `insert_tdb_table`, `query_tdb`: SQL operations
  - `list_tdb_tables`, `describe_tdb_table`: Schema inspection
  - `export_tdb_table`, `import_tdb_table`: Data portability (CSV, JSON, Parquet)
- **Legacy**: `db_storage.py` retains DuckDB utilities (deprecated)

**Vector Database (`lancedb_storage.py`, `vdb_tools.py`)**
- **Backend**: LanceDB with sentence-transformers embeddings
- **Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Purpose**: Long-term knowledge retrieval
- **Use Cases**: Meeting notes, decisions, documentation
- **Features**:
  - Semantic search (vector similarity)
  - Document chunking (configurable `chunk_size`)
  - Metadata filtering
- **Tools**:
  - `create_vdb_collection`: Create collection with embeddings
  - `add_vdb_documents`: Add documents (auto-chunking)
  - `search_vdb`: Semantic search
  - `vdb_list`, `drop_vdb_collection`: Collection management

**UserRegistry (`user_registry.py`)**
- **Purpose**: Conversation logs and ownership tracking
- **Features**:
  - Thread-scoped conversation history
  - Ownership tracking for files/TDB/VDB/reminders per thread
  - Message audit log for troubleshooting

**InstinctStorage (`instinct_storage.py`)**
- **Purpose**: Behavioral pattern learning with confidence scoring
- **Backend**: JSONL append-only log + compacted snapshot
- **Features**:
  - Confidence scoring (0.0-1.0, thresholds at 0.2/0.5/0.6)
  - 6 domains: communication, format, workflow, tool_selection, verification, timing
  - Automatic reinforcement and contradiction
  - Event-based audit trail
  - Thread-scoped storage
- **Tools**:
  - `create_instinct`: Manually create behavioral pattern
  - `list_instincts`: Show all learned patterns
  - `adjust_instinct_confidence`: Reinforce or contradict patterns
  - `get_applicable_instincts`: Find patterns matching context
  - `disable_instinct` / `enable_instinct`: Toggle patterns
  - `evolve_instincts`: Cluster patterns into draft skills
  - `approve_evolved_skill`: Save draft as user skill
  - `export_instincts` / `import_instincts`: Backup and sharing

**UserMCPStorage (`user_mcp_storage.py`)**
- **Purpose**: Per-conversation MCP server configuration management
- **Backend**: JSON files per-thread
- **Location**: `data/users/{thread_id}/mcp/`
- **Files**:
  - `mcp.json`: Local (stdio) server configurations
  - `mcp_remote.json`: Remote (HTTP/SSE) server configurations
  - Automatic backups: `mcp.json.backup_001` to `backup_005` (rotation)
- **Features**:
  - Server name validation (alphanumeric, underscore, hyphen only)
  - Command validation (stdio servers require command)
  - URL validation (HTTPS required, localhost exception for testing)
  - Security checks (command injection prevention)
  - Backup before modifications
  - Manual restore from any backup
- **Tools**:
  - `mcp_add_server`: Add local MCP server
  - `mcp_add_remote_server`: Add remote MCP server
  - `mcp_remove_server`: Remove server
  - `mcp_list_servers`: List all configured servers
  - `mcp_show_server`: Show server details
  - `mcp_export_config`: Export configuration as JSON
  - `mcp_import_config`: Import configuration from JSON
  - `mcp_list_backups`: List available backups
  - `mcp_restore_backup`: Restore from backup
  - `mcp_reload`: Reload tools from configuration

**MCPSkillStorage (`mcp_skill_storage.py`)**
- **Purpose**: HITL workflow for skill proposals from MCP servers
- **Backend**: JSON files per-thread
- **Location**: `data/users/{thread_id}/mcp/pending_skills/`
- **Proposal Schema**:
  ```json
  {
    "skill_name": "web_scraping",
    "source_server": "fetch",
    "reason": "The fetch tool requires knowledge of web scraping best practices",
    "content": "",
    "created_at": "2026-02-01T10:00:00Z",
    "status": "pending"  // pending | approved | rejected
  }
  ```
- **Features**:
  - Create pending proposals when MCP servers added
  - Approve/reject workflow with user control
  - Edit skill content before approving
  - List pending skills (sorted by created_at, newest first)
  - Get list of approved skills for loading
- **Functions**:
  - `save_pending_skill()`: Save proposal to storage
  - `load_pending_skill()`: Load proposal by name
  - `list_pending_skills()`: Get all pending proposals
  - `approve_skill()`: Mark as approved (loads on next reload)
  - `reject_skill()`: Mark as rejected
  - `get_approved_skills()`: Get list of approved skill names

**MCP Skill Mapping (`tools/mcp_skill_mapping.py`)**
- **Purpose**: Maps MCP servers to their associated skills
- **Database**: `MCP_SERVER_SKILLS` dictionary
- **Supported Servers**:
  - `fetch` â†’ `web_scraping`, `fetch_content` (web scraping best practices)
  - `github` â†’ `github_api`, `code_search`, `git_operations` (API patterns)
  - `clickhouse` â†’ `clickhouse_sql`, `database_queries` (SQL optimization)
  - `filesystem` â†’ `file_operations`, `file_security` (auto-load=False, requires paths)
  - `brave-search` â†’ `web_search`, `search_strategies` (query optimization)
  - `puppeteer` â†’ `browser_automation`, `web_scraping_advanced` (DOM manipulation)
- **Functions**:
  - `get_skills_for_server(name, command)`: Detect skills for a server
  - `get_skill_recommendation_reason(name)`: Get explanation
  - `is_server_auto_load(name)`: Check if skills should be auto-proposed

**Checkpoint (`checkpoint.py`)**
- **Purpose**: LangGraph state persistence
- **Backend**: PostgreSQL (via `langgraph-checkpoint-postgres`)
- **Tables**:
  - `checkpoints`: State snapshots per thread
  - `checkpoint_blobs`: Large message payloads
- **Alternative**: In-memory for development

**MetaRegistry (`meta_registry.py`)**
- **Purpose**: Ownership tracking for audit and data migration
- **Tables**:
  - `file_paths`: File ownership per thread
  - `tdb_paths`: Transactional database ownership per thread
  - `vdb_paths`: Vector database ownership per thread
  - `adb_paths`: Analytics DB ownership per thread
- **Operations**: Track all create/delete operations

**User Allowlist (`user_allowlist.py`)**
- **Purpose**: Channel-based access control
- **Implementation**:
  - HTTP channels: **Always authorized** (authentication handled by frontend)
  - Telegram channels: **Require allowlist** (anyone can message the bot)
  - Admin thread IDs: Always authorized (from `docker/config.yaml`)
- **File**: `data/admins/user_allowlist.json`
- **Format**: `{"users": ["telegram:123456", "telegram:789012"]}`
- **Rationale**:
  - HTTP: Frontend application handles authentication (JWT sessions, OAuth, etc.)
  - Telegram: Public platform, need allowlist to prevent unauthorized access
- **Pattern**: Follows LangGraph Studio dev/up model - auth is frontend responsibility

**Reminder (`reminder.py`, `scheduled_flows.py`)**
- **Purpose**: Scheduled notification system
- **Backend**: APScheduler (in-memory) + PostgreSQL persistence
- **Features**:
  - One-time and recurring reminders
  - Recurrence rules (daily, weekly, custom)
  - Multi-thread triggering (notify across conversations)
  - Timezone-aware scheduling
- **Table**: `reminders`

### Analytics DB (DuckDB)

- Context-scoped DuckDB for analytics queries.
- Stored at `data/users/{thread_id}/analytics/duckdb.db`.
- Tool: `query_adb` (read/write SQL for analysis).

### 4. Tools Layer (`src/executive_assistant/tools/`)

**Purpose:** LangChain tool implementations that the agent can invoke.

**Tool Registry (`registry.py`)**
- `get_all_tools()`: Aggregates all tool categories (87 total tools)
- **All tools available by default** - No progressive disclosure filtering
  - Token overhead: ~8,100 tokens (4% of 200K context)
  - Prevents multi-step workflow breakage
  - Removed: `get_tools_for_request()` (deprecated, caused tool loss mid-conversation)
- Categories:
  - File tools (11 tools): File operations
  - TDB tools (10 tools): Database operations
  - ADB tools (5 tools): Analytics database operations
  - VDB tools (7 tools): Vector database operations
  - Time tools (3 tools): Timezone queries
  - Reminder tools (4 tools): Reminder management
  - Python tools (2 tools): Code execution
  - Search tools (2 tools): Web search via SearXNG
  - Browser tools (1 tool): Playwright scraping
  - OCR tools (2 tools): Image/PDF text extraction
  - Firecrawl tools (3 tools): Web scraping
  - Agent tools (6 tools): Mini-agent creation and management
  - Flow tools (5 tools): Workflow automation
  - Memory tools (6 tools): Memory extraction and search
  - Meta tools (3 tools): System metadata
  - Instinct tools (13 tools): Behavioral pattern learning
  - MCP tools (14 tools): Configurable MCP server integration
    - Server management: add, remove, list, show servers (local + remote)
    - Configuration: export, import, list backups, restore backup
    - Hot-reload: clear MCP cache and reload tools
    - HITL workflow: list pending skills, approve, reject, edit, show skills
  - Confirmation tool (1 tool): Large operation confirmation
  - Skills tool (1 tool): Dynamic skill loading

**Python Tool (`python_tool.py`)**
- **Purpose**: Safe Python code execution for data processing
- **Security**:
  - 30-second timeout
  - Thread-scoped I/O (via `file_sandbox` paths)
  - Whitelisted modules: `json`, `csv`, `math`, `datetime`, `random`, `statistics`, `urllib`, `pandas`, `numpy`
  - Path traversal protection
- **Tools**:
  - `python_exec`: Execute Python code
  - `python_exec_file`: Execute Python file

**Time Tool (`time_tool.py`)**
- **Purpose**: Timezone-aware time queries
- **Tools**:
  - `get_current_time(timezone)`: Get time in specific timezone
  - `get_current_date(timezone)`: Get date in specific timezone
  - `list_timezones()`: List available timezones

**Search Tool (`search_tool.py`)**
- **Purpose**: Web search via SearXNG
- **Features**: Privacy-focused search aggregation
- **Config**: `SEARXNG_HOST` environment variable

**OCR Tool (`ocr_tool.py`)**
- **Purpose**: Extract text from images and PDFs
- **Engines**: Surya-OCR (default) or PaddleOCR (Linux x86_64 only)
- **Features**:
  - PDF text extraction (multi-page)
  - Image OCR (PNG, JPG, etc.)
  - Structured extraction with LLM (JSON output)
- **Config**: `ocr` section in `docker/config.yaml`

### 5. Skills System (`src/executive_assistant/skills/`)

**Purpose:** Progressive disclosure of advanced patterns through dynamic skill loading.

**Components:**
- `registry.py`: Skill registration
- `loader.py`: Load skill definitions from `.skill` files
- `builder.py`: Build LangGraph graphs from skills
- `tool.py`: Wrap skills as LangChain tools
- `content/`: Skill definition files

**Example Skill:** `react-agent.skill` (17KB)
- Defines ReAct agent pattern as a reusable skill
- Can be loaded via `load_skill` tool

### 6. Instincts System (`src/executive_assistant/instincts/`)

**Purpose:** Automatic behavioral pattern learning from user interactions.

**Components:**

**Observer (`observer.py`)**
- **Purpose**: Detect behavioral patterns in user messages
- **Pattern Types**:
  - Corrections: "Actually, I meant..." â†’ Apologize and adjust
  - Repetitions: "Do it again..." â†’ Follow same pattern
  - Verbosity: "Be concise" / "More detail" â†’ Adjust response length
  - Format: "Use JSON" / "Bullet points" â†’ Output format preference
- **Features**:
  - Regex-based pattern detection
  - Automatic reinforcement of existing patterns
  - Creates new instincts with 0.5-0.8 initial confidence
  - Integrated into message flow (BaseChannel.handle_message)

**Injector (`injector.py`)**
- **Purpose**: Load applicable instincts into system prompts
- **Features**:
  - Context-aware filtering (matches user message to triggers)
  - Confidence-based formatting (bold for â‰¥0.8, conditional for lower)
  - Fallback to all high-confidence instincts if no matches
  - Domain-specific sections (Communication, Format, Workflow, etc.)
- **Integration**: Called in `get_system_prompt()` between BASE_PROMPT and CHANNEL_APPENDIX

**Evolver (`evolver.py`)**
- **Purpose**: Cluster related instincts into reusable skills
- **Algorithm**:
  1. Group instincts by domain
  2. Extract keywords from triggers
  3. Find common themes (â‰¥2 instincts sharing keywords)
  4. Build cluster with avg confidence
  5. Generate draft skill with behavioral patterns
- **Requirements**: Minimum 2 instincts per cluster, â‰¥0.6 avg confidence
- **HITL**: Human-in-the-loop approval required (approve_evolved_skill tool)

**Profiles (`profiles.py`)**
- **Purpose**: Quick personality configuration with preset instincts
- **Available Profiles**:
  1. **Concise Professional**: Brief, business-focused (3 instincts)
  2. **Detailed Explainer**: Thorough with examples (3 instincts)
  3. **Friendly Casual**: Conversational, approachable (3 instincts)
  4. **Technical Expert**: Precise technical language (4 instincts)
  5. **Agile Developer**: Iterative, testing-focused (3 instincts)
  6. **Analyst Researcher**: Data-driven analysis (4 instincts)
- **Tools**:
  - `list_profiles`: Browse available profiles
  - `apply_profile`: Apply profile to current thread
  - `create_custom_profile`: Build custom personality pack

**System Prompt Assembly Order:**
```python
system_prompt = (
    BASE_PROMPT +              # "You are {AGENT_NAME}..."
    ADMIN_PROMPT +             # Admin policies (safety, etc.)
    INSTINCTS_SECTION +        # "## Behavioral Patterns\nApply these..."
    CHANNEL_APPENDIX           # "HTTP/Telegram Formatting..."
)
```

**Storage Schema:**
```json
{
  "id": "uuid",
  "trigger": "user asks quick questions",
  "action": "respond briefly, skip detailed explanations",
  "domain": "communication",
  "source": "session-observation",
  "confidence": 0.8,
  "status": "enabled",
  "created_at": "2026-01-31T10:00:00Z"
}
```

### 7. Configuration Layer (`src/executive_assistant/config/`)

**Purpose:** Centralized configuration management.

**Components:**
- `settings.py`: Pydantic Settings class (environment + YAML)
- `llm_factory.py`: LLM model creation (provider-agnostic)
- `loader.py`: Load config from `docker/config.yaml` and `docker/.env`
- `constants.py`: Application constants

**Configuration Sources** (priority order):
1. Environment variables (`docker/.env`)
2. `docker/config.yaml` (application defaults)
3. Pydantic defaults

**Key Settings:**
- `DEFAULT_LLM_PROVIDER`: LLM provider selection (anthropic, openai, zhipu, ollama)
- `CHECKPOINT_STORAGE`: postgres or memory
- `EXECUTIVE_ASSISTANT_CHANNELS`: Comma-separated list (telegram, http)
- `MAX_ITERATIONS`: Max ReAct loops (default: 20)
- Middleware thresholds (max_tokens, call limits, etc.)

### 7. Workflows Layer (`src/executive_assistant/workflows/`)

**Purpose:** Integration with external workflow engines (currently Temporal).

**Components:**
- `health.py`: Health check workflow
- `temporal_client.py`: Temporal client wrapper

**Usage:** Optional integration for long-running, durable workflows.

### 8. Scheduler (`scheduler.py`)

**Purpose:** APScheduler integration for reminder notifications.

**Features:**
- Async job scheduling
- Job persistence (optional)
- Graceful shutdown

---

## Data Flow

### Message Processing Flow

```
[User Message]
    â”‚
    â†“
[Channel Layer]  (TelegramChannel or HttpChannel)
    â”‚  â€¢ Normalize message to HumanMessage
    â”‚  â€¢ Set thread_id in ContextVars
    â”‚  â€¢ Acquire thread lock (prevent concurrent processing)
    â”‚  â€¢ Observe message for instinct patterns (non-blocking)
    â”‚
    â†“
[Middleware Stack] (LangChain)
    â”‚  â€¢ Summarization (if token limit exceeded)
    â”‚  â€¢ Call limits (check max calls per message)
    â”‚  â€¢ Status updates (enable streaming)
    â”‚  â€¢ Todo tracking
    â”‚
    â†“
[LangGraph ReAct Agent]
    â”‚
    â”œâ”€â†’ [System Prompt Assembly]
    â”‚      â€¢ BASE_PROMPT (core role)
    â”‚      â€¢ ADMIN_PROMPT (safety policies)
    â”‚      â€¢ INSTINCTS_SECTION (learned patterns)
    â”‚      â€¢ CHANNEL_APPENDIX (formatting)
    â”‚
    â”œâ”€â†’ [call_model Node]
    â”‚      â€¢ Load conversation history from state
    â”‚      â€¢ Invoke LLM with system prompt + messages
    â”‚      â€¢ Record timing (milliseconds)
    â”‚      â€¢ Check for tool_calls
    â”‚
    â”œâ”€â†’ [Route: Tools or End?]
    â”‚      â€¢ If tool_calls: go to tools
    â”‚
    â”œâ”€â†’ [call_tools Node]
    â”‚      â€¢ Execute tool functions
    â”‚      â€¢ Each tool:
    â”‚        â€¢ Read ContextVars (thread_id)
    â”‚        â€¢ Access storage (file, TDB, VDB)
    â”‚        â€¢ Return results
    â”‚
    â”œâ”€â†’ [increment_iterations Node]
    â”‚      â€¢ iterations += 1
    â”‚
    â””â”€â†’ [Loop back to call_model] or [END]
         â”‚
         â†“
[Middleware Stack]
    â”‚  â€¢ Final status update
    â”‚  â€¢ Call limit logging
    â”‚
    â†“
[Channel Layer]
    â”‚  â€¢ Stream AIMessage chunks (if streaming)
    â”‚  â€¢ Or send complete response
    â”‚  â€¢ Edit status message (if enabled)
    â”‚
    â†“
[Checkpoint Saver] (PostgreSQL)
    â”‚  â€¢ Save final state to checkpoints table
    â”‚  â€¢ Async write (non-blocking)
    â”‚
    â†“
[User receives response]
```

### Tool Execution Flow

```
[Agent calls tool]
    â”‚
    â†“
[ThreadContextMiddleware.awrap_tool_call()]
    â”‚  â€¢ Capture thread_id from ContextVar
    â”‚  â€¢ Set thread_id again (ensure propagation)
    â”‚  â€¢ Log any errors with full traceback
    â”‚
    â†“
[Tool function executes]
    â”‚
    â”œâ”€â†’ Read ContextVars (_thread_id) âœ“ (now works!)
    â”‚
    â”œâ”€â†’ Build scoped path:
    â”‚      â€¢ if scope="shared" â†’ data/shared/
    â”‚      â€¢ if scope="context" â†’ data/users/{thread_id}/
    â”‚
    â”œâ”€â†’ Check permissions:
    â”‚      â€¢ FileSandbox: path traversal protection
    â”‚      â€¢ MetaRegistry: ownership verification
    â”‚
    â”œâ”€â†’ Access storage backend:
    â”‚      â€¢ File: Local filesystem
    â”‚      â€¢ TDB: SQLite (context + shared)
    â”‚      â€¢ VDB: LanceDB (collection-scoped)
    â”‚
    â”œâ”€â†’ Record operation:
    â”‚      â€¢ MetaRegistry: Update ownership tracking
    â”‚      â€¢ UserRegistry: Update audit log
    â”‚
    â””â”€â†’ Return results to agent
```

### MCP-Skill HITL Flow

When users add MCP servers, the system automatically proposes relevant skills:

```
[User adds MCP server]
    â”‚
    â†“
[mcp_add_server tool]
    â”‚  â€¢ Validate server configuration
    â”‚  â€¢ Save to mcp.json or mcp_remote.json
    â”‚  â€¢ Check skill mapping database
    â”‚
    â”œâ”€â†’ [For each associated skill]
    â”‚      â€¢ Check if already approved
    â”‚      â€¢ Create MCPSkillProposal
    â”‚      â€¢ Save to pending_skills/{skill_name}.json
    â”‚      â€¢ Status: "pending"
    â”‚
    â†“
[User reviews proposals]
    â”‚
    â”œâ”€â†’ mcp_list_pending_skills()
    â”‚   â€¢ Shows all pending skills
    â”‚   â€¢ Displays source server and reason
    â”‚   â€¢ Sorted by created_at (newest first)
    â”‚
    â”œâ”€â†’ mcp_show_skill(skill_name)
    â”‚   â€¢ Shows skill details
    â”‚   â€¢ Displays current content
    â”‚   â€¢ Shows available actions
    â”‚
    â”œâ”€â†’ mcp_edit_skill(skill_name, content) [optional]
    â”‚   â€¢ Customize skill content
    â”‚   â€¢ Preserves status
    â”‚
    â”œâ”€â†’ mcp_approve_skill(skill_name)
    â”‚   â€¢ Changes status to "approved"
    â”‚   â€¢ Will load on next reload
    â”‚
    â””â”€â†’ mcp_reject_skill(skill_name)
        â€¢ Changes status to "rejected"
        â€¢ Won't be loaded
    â”‚
    â†“
[mcp_reload tool]
    â”‚  â€¢ clear_mcp_cache() - Clear tool cache
    â”‚  â€¢ Load tools from all configured servers
    â”‚  â€¢ get_approved_skills() - Get approved skill list
    â”‚  â€¢ For each approved skill:
    â”‚     â€¢ load_skill(skill_name)
    â”‚     â€¢ Inject skill content into agent context
    â”‚
    â†“
[Agent now has tools + expertise]
```

**Key Benefits:**
- **Transparency**: Users see exactly what skills will be loaded
- **Control**: Users approve/reject individual skills
- **Customization**: Users can edit skills before loading
- **Safety**: Skills require explicit approval
- **Context**: Skills teach agent how/when/why to use tools

---

## Storage Architecture

### PostgreSQL Schema

| Table | Purpose | Key Columns |
|-------|---------|-------------|
| `checkpoints` | LangGraph state snapshots | `thread_id`, `checkpoint_ns`, `checkpoint_id` |
| `checkpoint_blobs` | Large message payloads | `thread_id`, `checkpoint_id`, `blob` |
| `conversations` | Conversation metadata | `thread_id`, `created_at` |
| `messages` | Message audit log | `thread_id`, `message_id`, `role`, `content` |
| `file_paths` | File ownership tracking | `thread_id`, `path`, `created_at` |
| `tdb_paths` | TDB ownership tracking | `thread_id`, `tdb_path`, `created_at` |
| `vdb_paths` | VDB ownership tracking | `thread_id`, `vdb_path`, `created_at` |
| `adb_paths` | Analytics DB ownership tracking | `thread_id`, `adb_path`, `created_at` |
| `reminders` | Scheduled reminders | `reminder_id`, `thread_id`, `trigger_time`, `recurrence` |

**Note:** Instincts are stored in the filesystem (JSONL + snapshot) under `data/users/{thread_id}/instincts/`, not in PostgreSQL.

### Data Isolation Model

**Thread-Level Isolation (Default)**
- Each conversation gets unique `thread_id` (e.g., Telegram chat_id)
- Data stored under `data/users/{thread_id}/`
- Prevents cross-thread data leakage

**Organization-Level Sharing**
- Admins can write to `data/shared/`
- All users can read from shared storage
- Use cases: Company-wide knowledge, templates
 
Thread-only context
- Enables multi-thread access to user data

### Vector Database Architecture

**LanceDB Collections**
- Per-thread collections
- Embeddings via sentence-transformers
- Metadata filtering support

**Document Chunking**
- Split documents into ~3000 character chunks
- Overlap for context preservation
- Configurable via `vector_store.chunk_size`

**Search Options**
1. **Semantic Search**: Vector similarity (cosine distance)
2. **Full-Text Search**: Keyword matching
3. **Hybrid Search**: Combined score

---

## Key Libraries & Frameworks

### LangGraph (`langgraph >= 1.0.6`)
**Role:** Agent orchestration and state management
**Key Features Used:**
- `StateGraph`: Build custom agent graphs
- `add_messages` reducer: Automatic message history accumulation
- `BaseCheckpointSaver`: State persistence interface
- `RunnableConfig`: Per-invocation configuration

**Key Components:**
```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
```

### LangChain (`langchain >= 0.3.27`)
**Role:** Unified LLM and tool interface
**Key Features Used:**
- `BaseChatModel`: Provider-agnostic LLM interface
- `BaseTool`: Tool implementation base class
- Middleware stack: Summarization, retry, limits
- `StructuredTool`: Tools with JSON schemas

**Key Components:**
```python
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import BaseTool, tool
from langchain.agents.middleware import (
    SummarizationMiddleware,
    CallLimitMiddleware,
    TodoListMiddleware,
)
```

### LLM Providers

**Anthropic (`langchain-anthropic >= 0.3.22`)**
- Claude models (Haiku, Sonnet)
- Used by default in `docker/config.yaml`

**OpenAI (`langchain-openai >= 0.3.35`)**
- GPT models (GPT-4o, GPT-4o-mini)
- Alternative to Anthropic

**Zhipu AI (`zhipuai`)**
- GLM-4 models
- Chinese LLM provider

**Ollama (`langchain-ollama >= 0.3.1`)**
- Local models or Ollama Cloud
- Privacy-focused option

### FastAPI (`fastapi >= 0.115.0`)
**Role:** HTTP channel implementation
**Key Features Used:**
- ASGI async server
- Pydantic models for validation
- Server-Sent Events (SSE) streaming
- CORS support

**Key Components:**
```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
```

### APScheduler (`apscheduler >= 3.10.0`)
**Role:** Reminder scheduling
**Key Features Used:**
- Async job scheduling
- Cron-like recurrence rules
- Timezone-aware triggers

**Key Components:**
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
```

### DuckDB (`duckdb >= 1.1.0`) (legacy)
**Role:** Thread-scoped database
**Key Features Used:**
- Embedded, no external process needed
- SQL compatibility
- Fast in-memory queries
- CSV/JSON/Parquet import/export

### LanceDB (`lancedb >= 0.15.0`)
**Role:** Vector store for semantic search
**Key Features Used:**
- Embedded vector database
- Sentence-transformers integration
- Hybrid search (vector + full-text)
- Metadata filtering

### psycopg + asyncpg (`asyncpg >= 0.30.0`)
**Role:** PostgreSQL async driver
**Key Features Used:**
- Async connection pooling
- Prepared statements
- Type safety

### Pydantic (`pydantic >= 2.12.5`)
**Role:** Configuration and validation
**Key Features Used:**
- `BaseSettings`: Environment variable management
- Type validation
- YAML configuration loading

**Key Components:**
```python
from pydantic import BaseModel, Field, BaseSettings
from pydantic_settings import BaseSettings
```

### Loguru (`loguru >= 0.7.2`)
**Role:** Structured logging
**Key Features Used:**
- JSON logging
- Log rotation
- Millisecond timestamps
- Context-aware logging

### Python Standard Library
- `asyncio`: Async runtime
- `pathlib`: Path manipulation
- `contextvars`: Thread-scoped context
- `datetime`, `timezone`: Time handling

---

## Configuration Management

### Configuration Files

**`docker/config.yaml`** - Application Defaults
- LLM provider settings
- Storage paths
- Agent parameters
- Middleware thresholds
- Vector store settings
- OCR configuration

**`docker/.env`** - Environment-Specific Overrides
- API keys (secrets)
- Channel configuration
- Database credentials
- External service URLs

**Admin Customization (`data/admins/`)**
- `prompts/prompt.md`: Prepended before the system prompt (admin-only).
- `skills/`: Loaded as additional skills at startup (admin-only).
- `mcp.json`: Loaded when MCP adapters are available (admin-only).
- `user_allowlist.json`: Access control list managed by admins.

**`pyproject.toml`** - Project Metadata
- Dependencies
- Build configuration
- Scripts (`executive_assistant`, `executive_assistant-dev`)

### Configuration Loading

1. **Load `docker/config.yaml`** â†’ `Settings` object
2. **Override with `docker/.env`** â†’ Environment variables
3. **Validate with Pydantic** â†’ Type checking
4. **Use in application** â†’ `settings` singleton

**Example:**
```python
from executive_assistant.config import settings

# Access settings
provider = settings.DEFAULT_LLM_PROVIDER
max_iter = settings.MAX_ITERATIONS
```

### LLM Factory

**`llm_factory.py`** creates provider-agnostic models:
```python
from executive_assistant.config import create_model

# Creates model based on DEFAULT_LLM_PROVIDER
model = create_model()  # Returns BaseChatModel
```

**Supported Providers:**
- `anthropic`: Claude models
- `openai`: GPT models
- `zhipu`: GLM-4 models
- `ollama`: Local/Cloud models

---

## Token Usage Tracking

Executive Assistant monitors token consumption to control costs and understand usage patterns:

### Implementation

**HTTP Channel Token Tracking** (`channels/http.py`):
- Extracts `usage_metadata` from AIMessage objects in the event stream
- Logs input/output/total tokens per conversation: `tokens={input}+{output}={total}`
- **Provider Support**:
  - âœ… OpenAI: Full token tracking (input + output + total)
  - âœ… Anthropic: Full token tracking
  - âŒ Ollama: No metadata provided (usage not tracked)

### Token Breakdown

Typical token usage for a conversation:

| Component | Token Count | Notes |
|-----------|-------------|-------|
| System prompt | ~50 tokens | "You are Jen, a personal AI assistant..." |
| Tools (87 tools) | ~8,100 tokens | Tool names, descriptions, JSON schemas |
| Instincts (variable) | ~100-500 tokens | Learned behavioral patterns |
| Conversation messages | Variable | Grows with each turn (Â±30-80 tokens per round) |
| **Total (Round 1)** | ~8,250 tokens | System + tools + first user message |
| **Total (Round 5)** | ~8,700 tokens | +450 tokens from 4 conversation turns |

### Example Log Output

```bash
CH=http CONV=http_user123 TYPE=token_usage | message tokens=7581+19=7600
CH=http CONV=http_user123 TYPE=token_usage | message tokens=7900+808=8708
CH=http CONV=http_user123 TYPE=token_usage | message tokens=8726+803=9529
```

**Interpretation**:
- Input tokens grow as conversation context is preserved
- Output tokens vary based on response complexity
- Cache hits (OpenAI) reduce effective token cost significantly

### Summarization Middleware

**Configuration** (`docker/config.yaml`):
```yaml
middleware:
  summarization:
    enabled: true
    max_tokens: 5000     # Trigger: 5,000 conversation messages
    target_tokens: 1000  # Target: 1,000 messages after summarization
```

**Important Notes**:
- Threshold applies to **conversation messages only**, not total LLM input
- Does NOT count the ~8,100 token system prompt + tools + instincts overhead
- With 128K context window (GPT-OSS 20B), 5,000 messages is very conservative
- Summarization calls the LLM to compress older messages into a summary

**When to Adjust**:
- **Lower threshold** (500-1,000): For faster summarization, more aggressive compression
- **Higher threshold** (10,000+): For longer conversations before summarization
- **Use fractional triggering**: `trigger: 0.4` (40% of model context window)

---

## Deployment Architecture

### Docker Deployment

**`docker/Dockerfile`** - Multi-stage build
- Base: `python:3.13-slim`
- Dependencies: `uv sync --frozen`
- User: `executive_assistant` (non-root)
- Ports: 8000
- Volumes: `/app/data`

**`docker/docker-compose.yml`** - Development stack
```yaml
services:
  executive_assistant:    # Agent application
  postgres:  # PostgreSQL for state persistence
```

### Production Deployment

**Environment Variables Required:**
```bash
# LLM Provider
DEFAULT_LLM_PROVIDER=anthropic  # or openai, zhipu, ollama
ANTHROPIC_API_KEY=sk-ant-xxx

# Channels
EXECUTIVE_ASSISTANT_CHANNELS=telegram,http

# Telegram
TELEGRAM_BOT_TOKEN=your-bot-token

# PostgreSQL
POSTGRES_HOST=postgres
POSTGRES_PASSWORD=executive_assistant_password

# Optional: External Services
SEARXNG_HOST=https://your-searxng.com
FIRECRAWL_API_KEY=fc-xxx
```

### Development Workflow

**Local Testing (Recommended):**
```bash
# Start PostgreSQL
docker compose up -d postgres

# Run locally (no Docker rebuild needed)
uv run executive_assistant

# Run HTTP only
EXECUTIVE_ASSISTANT_CHANNELS=http uv run executive_assistant
```

**Docker Deployment:**
```bash
# Build and run all services
docker compose up -d

# Rebuild executive_assistant container
docker compose build --no-cache executive_assistant
```

### Scaling Considerations

**Stateless Channels:**
- HTTP channel can scale horizontally (multiple instances)
- Load balancer distributes traffic

**Stateful Agent:**
- PostgreSQL checkpointing enables persistence
- Multiple instances share state via PostgreSQL

**Storage:**
- File storage: Use networked volume (NFS, S3, etc.)
- Vector store: LanceDB embedded (one instance per agent)

---

## Testing Strategy

### Test Framework
- **pytest v9.0.2+**: Test runner
- **pytest-asyncio v1.3.0+**: Async test support
- **pytest-recording**: HTTP request recording/replay

### Test Categories

**Unit Tests**
- `test_file_sandbox.py`: File operations
- `test_db_storage.py`: DuckDB operations (legacy)
- `test_lancedb_vdb.py`: Vector database operations
- `test_python_tool.py`: Python execution

**Integration Tests**
- `test_agent.py`: Agent execution with mock LLM
- `test_status_middleware.py`: Middleware behavior
- `test_temporal_api.py`: Temporal integration

**System Tests**
- `test_integration_pg.py`: Full PostgreSQL integration
- `test_scheduled_flows.py`: Reminder scheduling

### Test Fixtures (`conftest.py`)
- Mock LLM responses
- Test data generation
- Async event loop management

### VCR Cassettes
- Record live LLM calls for replay in tests
- Avoid hitting rate limits
- Enable deterministic tests

**Recording:**
```bash
RUN_LIVE_LLM_TESTS=1 uv run pytest -m "langchain_integration and vcr" --record-mode=once
```

---

## Appendix: Key Concepts

### ReAct Pattern
The agent follows the **ReAct** (Reasoning + Acting) pattern:
1. **Reason**: LLM analyzes request and decides what to do
2. **Act**: Execute tools to gather information or perform actions
3. **Observe**: Process results and update context
4. **Loop**: Repeat until task complete

### Context Variables
Python `ContextVar` provides thread-scoped context:
```python
_thread_id: ContextVar[str] = ContextVar("_thread_id", default=None)
```

This ensures:
- Thread isolation (no cross-thread data leakage)
- Async propagation (works with asyncio)
- Zero performance overhead

### Middleware Chain
LangChain middleware wraps tool/model calls:
```
[User Request]
  â†’ [TodoListMiddleware] (track planned tasks)
  â†’ [SummarizationMiddleware] (reduce context if needed)
  â†’ [CallLimitMiddleware] (check call limits)
  â†’ [RetryMiddleware] (retry on failure)
  â†’ [Model/Tool]
  â†’ [StatusUpdateMiddleware] (stream progress)
  â†’ [TodoListDisplayMiddleware] (show todos)
  â†’ [User Response]
```

### Checkpointing
LangGraph saves state after each node execution:
- Resume from checkpoint after interruption
- Multi-session conversations
- Debug agent decisions

### Tool Selection
The LLM chooses tools dynamically based on:
- Request context
- Available tools
- Tool descriptions and schemas

This enables:
- Context-aware behavior
- Dynamic tool chains
- Multi-step reasoning

---

## Document Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.3.0 | 2026-02-01 | Implemented User MCP Management and MCP-Skill HITL Integration |
| 1.2.0 | 2026-01-31 | Implemented Instinct System (Observer, Injector, Evolver, Profiles) |
| 1.1.0 | 2026-01-28 | Added ThreadContextMiddleware, HTTP auth bypass, error logging enhancements |
| 1.0.0 | 2026-01-21 | Initial technical architecture documentation |

### Key Changes in v1.3.0

**New Feature: User MCP Management**
- Per-conversation MCP server configuration (separate from admin MCP)
- Support for stdio (command-line) and HTTP/SSE (remote) servers
- Tiered loading: User tools override admin tools when names conflict
- Tool deduplication: Prevents duplicate tools with same name
- Hot-reload: `clear_mcp_cache()` for updating tools without restart
- Security validation:
  - Server name validation (alphanumeric, underscore, hyphen only)
  - Command injection prevention (validates command format)
  - HTTPS enforcement for remote servers (localhost exception)
  - JSON validation for env/headers arguments
- Backup/Restore:
  - Automatic backups before modifications (keeps last 5)
  - Manual restore from any backup point
  - Rotation: Oldest backup deleted when exceeding limit
- Storage: `data/users/{thread_id}/mcp/mcp.json` and `mcp_remote.json`

**New Feature: MCP-Skill HITL Integration**
- Skill mapping database: Maps MCP servers to associated skills
- Auto-detection: When adding servers, relevant skills are proposed
- Human-in-the-loop workflow:
  - `mcp_list_pending_skills`: Show all proposals
  - `mcp_show_skill`: View skill details before deciding
  - `mcp_approve_skill`: Approve skill (loads on next reload)
  - `mcp_reject_skill`: Reject skill (won't be loaded)
  - `mcp_edit_skill`: Customize skill content
- Storage: `data/users/{thread_id}/mcp/pending_skills/{skill_name}.json`
- Enhanced tools:
  - `mcp_add_server`: Now creates skill proposals
  - `mcp_reload`: Now loads approved skills into context
- Supported servers: fetch, github, clickhouse, filesystem, brave-search, puppeteer

**Architecture Improvements:**
- Separation of concerns: MCP config vs skill proposals
- Status workflow: pending â†’ approved/rejected
- Proposal metadata: source_server, reason, created_at
- Audit trail: All proposals stored with timestamps
- Thread isolation: Each conversation has its own servers and skills

**Testing:**
- 33 tests for storage and skill mapping (`test_mcp_skill_hitl.py`)
- 27 tests for HITL workflow tools (`test_mcp_hitl_tools.py`)
- Total: 60 tests, all passing
- Test coverage: Proposal lifecycle, mapping detection, tool integration, error handling

**New Files:**
- `src/executive_assistant/storage/mcp_skill_storage.py` (130+ lines)
- `src/executive_assistant/tools/mcp_skill_mapping.py` (117 lines)
- `src/executive_assistant/tools/user_mcp_tools.py` (700+ lines, enhanced)
- `tests/test_mcp_skill_hitl.py` (400+ lines)
- `tests/test_mcp_hitl_tools.py` (500+ lines)

**Tool Count:**
- Increased from 87 to 101 tools (+14 MCP management tools)

### Key Changes in v1.2.0

**New Feature: Instinct System**
- Automatic behavioral pattern learning from user interactions
- Observer: Detects corrections, repetitions, verbosity/format preferences
- Injector: Loads applicable instincts into system prompts
- Evolver: Clusters related instincts into reusable skills
- Profiles: 6 pre-built personality presets (Concise Professional, Detailed Explainer, etc.)
- 13 instinct tools for management and evolution
- JSONL + snapshot storage for auditability

**Architecture Improvements:**
- System prompt assembly now includes instincts between BASE_PROMPT and CHANNEL_APPENDIX
- Observer integrated into message flow (non-blocking pattern detection)
- Confidence scoring (0.0-1.0) with automatic thresholds
- Human-in-the-loop approval for skill evolution

**Tool Count:**
- Increased from 83 to 87 tools (+4 instinct management tools)

### Key Changes in v1.1.0

**Bug Fixes:**
- Fixed progressive disclosure bug causing tool loss mid-conversation
- Fixed thread_id ContextVar not propagating to tool execution
- Fixed HTTP channel non-streaming endpoint (missing `stream_agent_response` method)
- Enhanced error logging with full tracebacks at DEBUG level

**Architecture Improvements:**
- All 87 tools now available by default (~8,100 tokens = 4% of 200K context)
- HTTP channel bypasses allowlist (frontend authentication pattern)
- ThreadContextMiddleware ensures context propagation across async boundaries
- Removed deprecated `get_tools_for_request()` function

**New Components:**
- `src/executive_assistant/agent/thread_context_middleware.py` - Context propagation middleware
- `src/executive_assistant/instincts/` - Complete instinct learning system (observer, injector, evolver, profiles)
- Enhanced `is_authorized()` in `user_allowlist.py` - Channel-specific authorization

---

**Document Author:** Generated via analysis of Executive Assistant codebase
**Contact:** See repository for issues and contributions
