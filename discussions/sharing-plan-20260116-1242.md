# Sharing Plan (Files/KB/DB) + Complexity Assessment

## Goal
Allow users to share some or all of their files, KB, and DB with other users, with explicit read/write permissions and auditability.

---

## Plan (Updated with Technical Details)

### 1) Share Registry (Source of Truth)
**Store in Postgres** (not Redis/Valkey).

#### Schema

```sql
CREATE TABLE share_registry (
    -- Primary key
    share_id          UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Owner
    owner_user_id     TEXT NOT NULL,
    owner_thread_id   TEXT NOT NULL,

    -- Resource
    resource_type     TEXT NOT NULL CHECK (resource_type IN ('file', 'kb', 'db')),
    resource_ref      TEXT NOT NULL,  -- See §1a format spec
    scope             TEXT NOT NULL CHECK (scope IN ('whole', 'subset')),
    subset_filter     JSONB,          -- Optional: for subset scope (e.g., {"rows": "id > 100"})

    -- Permissions
    permission        TEXT NOT NULL CHECK (permission IN ('read', 'write')),
    target_user_id    TEXT NOT NULL,

    -- Lifecycle
    status            TEXT NOT NULL CHECK (status IN ('pending', 'active', 'revoked', 'expired')),
    created_at        TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    expires_at        TIMESTAMPTZ,    -- NULL = no expiration
    activated_at      TIMESTAMPTZ,
    revoked_at        TIMESTAMPTZ,
    revoked_by        TEXT,           -- user_id who revoked

    -- Metadata
    description       TEXT,           -- Human-readable share description
    access_count      INTEGER DEFAULT 0,
    last_accessed_at  TIMESTAMPTZ,

    -- Constraints
    CONSTRAINT valid_expiration CHECK (expires_at IS NULL OR expires_at > created_at),
    CONSTRAINT target_not_owner CHECK (target_user_id != owner_user_id)
);

-- Indexes for common queries
CREATE INDEX idx_shares_owner ON share_registry(owner_user_id, status);
CREATE INDEX idx_shares_target ON share_registry(target_user_id, status);
CREATE INDEX idx_shares_resource ON share_registry(resource_type, resource_ref);
CREATE INDEX idx_shares_expiration ON share_registry(status, expires_at) WHERE expires_at IS NOT NULL;

-- Cascade revocation: trigger to revoke shares when resource is marked deleted
CREATE OR REPLACE FUNCTION revoke_deleted_resource_shares()
RETURNS TRIGGER AS $$
BEGIN
    UPDATE share_registry
    SET status = 'revoked',
        revoked_at = NOW(),
        revoked_by = 'system'
    WHERE resource_ref = OLD.resource_ref
      AND resource_type = OLD.resource_type
      AND status IN ('pending', 'active');
    RETURN OLD;
END;
$$ LANGUAGE plpgsql;

-- Call this trigger when a resource is soft-deleted
-- CREATE TRIGGER resource_deleted_trigger ... (implementation-specific)
```

#### 1a) `resource_ref` Format Specification

Use **stable identifiers** that don't change when resources move.

```python
# Format: "{prefix}:{owner_id}:{resource_id}"
# Examples by resource type:

FILE_REF_PATTERN = "file:{owner_user_id}:{file_uuid}"
# Example: "file:user_abc:123e4567-e89b-12d3-a456-426614174000"

KB_REF_PATTERN = "kb:{owner_user_id}:{table_name}"
# Example: "kb:user_abc:research_notes"
# For subset: "kb:user_abc:research_notes?filter=topic=AI"

DB_REF_PATTERN = "db:{owner_user_id}:{table_or_view_name}"
# Example: "db:user_abc:analytics_data"
# For subset: "db:user_abc:analytics_data?filter=date>2024-01-01"

# Legacy path format (deprecated, migrate to stable IDs)
LEGACY_REF_PATTERN = "path:{thread_id}:{relative_path}"
```

**Validation function**:
```python
import re
from uuid import UUID

def validate_resource_ref(resource_type: str, resource_ref: str) -> bool:
    patterns = {
        "file": r"^file:[^:]+:[0-9a-f-]{36}$",  # UUID
        "kb": r"^kb:[^:]+:[a-zA-Z_][a-zA-Z0-9_]*(\?.*)?$",
        "db": r"^db:[^:]+:[a-zA-Z_][a-zA-Z0-9_]*(\?.*)?$",
    }
    return re.match(patterns.get(resource_type, ""), resource_ref) is not None
```

---

### 2) Central Authorization Gate

#### Signature

```python
from typing import Literal, Optional
from dataclasses import dataclass

@dataclass
class AuthResult:
    allowed: bool
    share_id: Optional[str] = None
    reason: Optional[str] = None
    permission: Optional[Literal["read", "write"]] = None

def authorize(
    resource_type: Literal["file", "kb", "db"],
    resource_ref: str,
    action: Literal["read", "write"],
    caller_user_id: str,
    conn: asyncpg.Connection,  # Postgres connection
) -> AuthResult:
    """
    Central authorization gate for ALL shared resource access.

    Returns:
        AuthResult with allow/deny decision and context.

    Authorization Logic:
    1. Find active share for caller_user_id + resource
    2. Check if permission >= action (write >= read)
    3. Check if not expired
    4. Update access_count and last_accessed_at
    5. Return share_id for audit logging
    """
```

#### Implementation

```python
async def authorize(
    resource_type: Literal["file", "kb", "db"],
    resource_ref: str,
    action: Literal["read", "write"],
    caller_user_id: str,
    conn: asyncpg.Connection,
) -> AuthResult:
    # Find matching share
    row = await conn.fetchrow("""
        SELECT share_id, permission, expires_at, status
        FROM share_registry
        WHERE resource_type = $1
          AND resource_ref = $2
          AND target_user_id = $3
          AND status = 'active'
    """, resource_type, resource_ref, caller_user_id)

    if not row:
        return AuthResult(allowed=False, reason="No active share found")

    # Check expiration
    if row['expires_at'] and row['expires_at'] < datetime.now(timezone.utc):
        # Auto-expire
        await conn.execute("""
            UPDATE share_registry
            SET status = 'expired'
            WHERE share_id = $1
        """, row['share_id'])
        return AuthResult(allowed=False, reason="Share expired")

    # Check permission level
    perm_required = {"read": 1, "write": 2}[action]
    perm_granted = {"read": 1, "write": 2}[row['permission']]
    if perm_granted < perm_required:
        return AuthResult(
            allowed=False,
            reason=f"Insufficient permission: have {row['permission']}, need {action}"
        )

    # Update access stats
    await conn.execute("""
        UPDATE share_registry
        SET access_count = access_count + 1,
            last_accessed_at = NOW()
        WHERE share_id = $1
    """, row['share_id'])

    return AuthResult(
        allowed=True,
        share_id=str(row['share_id']),
        permission=row['permission']
    )
```

**MANDATORY**: All shared access tools MUST call `authorize()` first.

---

### 3) Tools

#### 3a) Read-Only Sharing (Phase 1)

| Tool | Owner | Target | Description |
|------|-------|--------|-------------|
| `share_resource` | ✅ | ❌ | Grant read access to a resource |
| `list_pending_shares` | ❌ | ✅ | List pending shares for caller |
| `accept_share` | ❌ | ✅ | Activate a pending share |
| `decline_share` | ❌ | ✅ | Decline a pending share |
| `list_shares` | ✅ | ✅ | List active shares (owned or received) |
| `read_shared_file` | ❌ | ✅ | Read a shared file |
| `search_shared_kb` | ❌ | ✅ | Search shared KB tables |
| `query_shared_db` | ❌ | ✅ | Query shared DB tables/views |
| `revoke_share` | ✅ | ❌ | Revoke a share (sets status=revoked) |

#### 3b) Write Sharing (Phase 2)

| Tool | Owner | Target | Description |
|------|-------|--------|-------------|
| `write_shared_file` | ❌ | ✅ | Write to a shared file (queued) |
| `add_shared_kb_documents` | ❌ | ✅ | Add docs to shared KB (queued) |
| `insert_shared_db_table` | ❌ | ✅ | Insert into shared DB (queued) |
| `get_write_job_status` | ✅ | ✅ | Check queued write status |
| `list_write_jobs` | ✅ | ✅ | List user's write jobs |

#### Tool Implementation Example

```python
@tool
async def read_shared_file(
    file_ref: str,
    caller_user_id: str,
    conn: asyncpg.Connection,
) -> str:
    """
    Read a file shared with you.

    Args:
        file_ref: The file reference (e.g., "file:user_abc:uuid")
        caller_user_id: Your user ID

    Returns:
        File content or error message.
    """
    # 1. Authorize
    auth = await authorize("file", file_ref, "read", caller_user_id, conn)
    if not auth.allowed:
        raise PermissionError(auth.reason)

    # 2. Log access
    await log_access(auth.share_id, caller_user_id, "read", file_ref, conn)

    # 3. Read file (implement based on storage layer)
    content = await read_file_from_storage(file_ref, caller_user_id)
    return content
```

---

### 4) Resource Mapping

| Type | Share Unit | Recommended | Notes |
|------|------------|-------------|-------|
| **Files** | Single file or folder | ✅ | Use UUID-based refs |
| **KB** | DuckDB table | ✅ | Share table, not entire DB file |
| **DB** | Table or Postgres view | ✅ | Views allow row-level filtering |

**View creation for subset sharing**:
```sql
-- Create a view for filtered access
CREATE VIEW shared_analytics_2024 AS
SELECT * FROM analytics_data
WHERE date >= '2024-01-01';

-- Share the view, not the base table
```

---

### 5) Audit Logging

#### Schema

```sql
CREATE TABLE share_access_log (
    log_id        BIGSERIAL PRIMARY KEY,
    share_id      UUID NOT NULL REFERENCES share_registry(share_id),
    caller_user_id TEXT NOT NULL,
    action        TEXT NOT NULL,  -- read, write, queue, revoke
    resource_ref  TEXT NOT NULL,
    status        TEXT NOT NULL,  -- success, denied, error
    error_message TEXT,
    ip_address    INET,
    user_agent    TEXT,
    created_at    TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Index for analytics queries
CREATE INDEX idx_access_log_share ON share_access_log(share_id, created_at DESC);
CREATE INDEX idx_access_log_user ON share_access_log(caller_user_id, created_at DESC);
CREATE INDEX idx_access_log_action ON share_access_log(action, created_at DESC);
```

#### Audit Function

```python
async def log_access(
    share_id: str,
    caller_user_id: str,
    action: str,
    resource_ref: str,
    conn: asyncpg.Connection,
    status: str = "success",
    error_message: str = None,
):
    await conn.execute("""
        INSERT INTO share_access_log
        (share_id, caller_user_id, action, resource_ref, status, error_message)
        VALUES ($1, $2, $3, $4, $5, $6)
    """, share_id, caller_user_id, action, resource_ref, status, error_message)
```

**Rate limiting based on audit logs**:
```python
async def check_rate_limit(
    caller_user_id: str,
    window_seconds: int = 60,
    max_requests: int = 100,
    conn: asyncpg.Connection,
) -> bool:
    count = await conn.fetchval("""
        SELECT COUNT(*)
        FROM share_access_log
        WHERE caller_user_id = $1
          AND created_at > NOW() - INTERVAL '1 second' * $2
    """, caller_user_id, window_seconds)
    return count < max_requests
```

---

### 6) Write Serialization (Queue + LISTEN/NOTIFY)

**Required** for shared DB writes to avoid DuckDB single-writer contention.

#### Queue Schema

```sql
CREATE TABLE shared_db_write_queue (
    job_id          UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    share_id        UUID NOT NULL REFERENCES share_registry(share_id),
    resource_ref    TEXT NOT NULL,

    -- Operation details
    operation_type  TEXT NOT NULL,  -- insert, update, delete
    payload         JSONB NOT NULL,  -- Operation-specific data

    -- Status tracking
    status          TEXT NOT NULL CHECK (status IN ('pending', 'processing', 'success', 'failed')),
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    started_at      TIMESTAMPTZ,
    completed_at    TIMESTAMPTZ,

    -- Result
    result          JSONB,
    error_message   TEXT,
    retry_count     INTEGER DEFAULT 0,
    max_retries     INTEGER DEFAULT 3,

    -- Requester
    requested_by    TEXT NOT NULL
);

-- Index for worker queries
CREATE INDEX idx_write_queue_status ON shared_db_write_queue(status, created_at)
    WHERE status IN ('pending', 'processing');
```

#### Worker Implementation

```python
import asyncio
import asyncpg

class WriteQueueWorker:
    def __init__(self, pg_dsn: str):
        self.pg_dsn = pg_dsn
        self.conn: Optional[asyncpg.Connection] = None
        self.running = False

    async def start(self):
        self.conn = await asyncpg.connect(self.pg_dsn)
        self.running = True

        # CRITICAL: Drain pending jobs on startup
        await self._drain_pending()

        # Listen for notifications
        await self.conn.add_listener('write_queue_notify', self._on_notify)

        # Start processing loop
        asyncio.create_task(self._process_loop())

    async def _drain_pending(self):
        """
        CRITICAL: On worker restart, drain all pending jobs.
        This prevents jobs from being stuck if worker crashes.
        """
        pending = await self.conn.fetchval("""
            SELECT COUNT(*) FROM shared_db_write_queue
            WHERE status = 'pending'
        """)
        if pending > 0:
            logger.info(f"Draining {pending} pending jobs on startup")
            # Process loop will pick them up

    async def _on_notify(self, connection, pid, channel, payload):
        """Woken by LISTEN/NOTIFY when new jobs arrive."""
        logger.debug(f"Received notification: {payload}")

    async def _process_loop(self):
        while self.running:
            try:
                # Fetch next pending job (FOR UPDATE SKIP LOCKED for concurrent safety)
                row = await self.conn.fetchrow("""
                    SELECT job_id, share_id, resource_ref, operation_type, payload
                    FROM shared_db_write_queue
                    WHERE status = 'pending'
                    ORDER BY created_at ASC
                    LIMIT 1
                    FOR UPDATE SKIP LOCKED
                """)

                if row:
                    await self._process_job(row)
                else:
                    # No jobs, wait for notification
                    await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"Error in process loop: {e}")
                await asyncio.sleep(5)

    async def _process_job(self, row):
        job_id = row['job_id']

        # Mark as processing
        await self.conn.execute("""
            UPDATE shared_db_write_queue
            SET status = 'processing', started_at = NOW()
            WHERE job_id = $1
        """, job_id)

        try:
            # Execute operation (implementation-specific)
            result = await self._execute_operation(row)

            # Mark success
            await self.conn.execute("""
                UPDATE shared_db_write_queue
                SET status = 'success',
                    completed_at = NOW(),
                    result = $1
                WHERE job_id = $2
            """, json.dumps(result), job_id)

        except Exception as e:
            # Check retry count
            retry_count = await self.conn.fetchval("""
                SELECT retry_count FROM shared_db_write_queue WHERE job_id = $1
            """, job_id)

            if retry_count < 3:
                # Requeue for retry
                await self.conn.execute("""
                    UPDATE shared_db_write_queue
                    SET status = 'pending',
                        retry_count = retry_count + 1,
                        error_message = $1
                    WHERE job_id = $2
                """, str(e), job_id)
            else:
                # Mark as permanently failed
                await self.conn.execute("""
                    UPDATE shared_db_write_queue
                    SET status = 'failed',
                        completed_at = NOW(),
                        error_message = $1
                    WHERE job_id = $2
                """, str(e), job_id)

    async def _execute_operation(self, row) -> dict:
        """Execute the actual write operation on DuckDB."""
        # Implementation depends on operation_type
        # Return result dict for logging
        pass
```

#### Enqueue in Tools

```python
@tool
async def insert_shared_db_table(
    table_ref: str,
    rows: list[dict],
    caller_user_id: str,
    conn: asyncpg.Connection,
) -> dict:
    """
    Insert rows into a shared DB table.

    Returns job_id for status tracking.
    """
    # 1. Authorize
    auth = await authorize("db", table_ref, "write", caller_user_id, conn)
    if not auth.allowed:
        raise PermissionError(auth.reason)

    # 2. Enqueue
    job_id = await conn.fetchval("""
        INSERT INTO shared_db_write_queue
        (share_id, resource_ref, operation_type, payload, requested_by)
        VALUES ($1, $2, 'insert', $3, $4)
        RETURNING job_id
    """, auth.share_id, table_ref, json.dumps({"rows": rows}), caller_user_id)

    # 3. Notify worker
    await conn.execute("NOTIFY write_queue_notify")

    # 4. Log
    await log_access(auth.share_id, caller_user_id, "queue", table_ref, conn)

    return {
        "job_id": str(job_id),
        "status": "queued",
        "check_status_tool": "get_write_job_status"
    }
```

#### Write Job Status Check

```python
@tool
async def get_write_job_status(
    job_id: str,
    caller_user_id: str,
    conn: asyncpg.Connection,
) -> dict:
    """Check the status of a queued write job."""
    row = await conn.fetchrow("""
        SELECT job_id, status, created_at, started_at, completed_at,
               result, error_message, retry_count
        FROM shared_db_write_queue
        WHERE job_id = $1 AND requested_by = $2
    """, job_id, caller_user_id)

    if not row:
        raise ValueError("Job not found")

    return {
        "job_id": str(row['job_id']),
        "status": row['status'],
        "created_at": row['created_at'].isoformat(),
        "started_at": row['started_at'].isoformat() if row['started_at'] else None,
        "completed_at": row['completed_at'].isoformat() if row['completed_at'] else None,
        "result": row['result'],
        "error": row['error_message'],
        "retry_count": row['retry_count']
    }
```

---

### 7) Optional Caching

Use **Postgres as source of truth**.
Add **in-process TTL cache** (30–120s) only if performance profiling shows a bottleneck.

```python
from functools import lru_cache
from datetime import datetime, timedelta

class ShareCache:
    def __init__(self, ttl_seconds: int = 60):
        self.ttl = timedelta(seconds=ttl_seconds)
        self._cache: dict[str, tuple[AuthResult, datetime]] = {}

    def get(self, key: str) -> Optional[AuthResult]:
        if key in self._cache:
            result, timestamp = self._cache[key]
            if datetime.now() - timestamp < self.ttl:
                return result
            del self._cache[key]
        return None

    def set(self, key: str, result: AuthResult):
        self._cache[key] = (result, datetime.now())

# Cache key format: "{resource_type}:{resource_ref}:{caller_user_id}"
```

**Redis/Valkey is NOT required initially**—add later if QPS > 1000 or multi-instance deployment.

---

### 8) Rate Limiting

Protect against abuse:

| Limit | Default | Rationale |
|-------|---------|-----------|
| Share creation | 10/hour per user | Prevent spam |
| Share access | 100/minute per user | Prevent scraping |
| Write queue | 50/hour per user | Prevent queue flooding |
| Pending shares | 100 per user | Prevent resource exhaustion |

**Implementation** (using audit logs):
```python
async def check_rate_limit(
    caller_user_id: str,
    action: str,
    conn: asyncpg.Connection,
) -> tuple[bool, str | None]:
    """Return (allowed, error_message)."""
    limits = {
        "share": (10, 3600),      # 10 per hour
        "access": (100, 60),      # 100 per minute
        "queue": (50, 3600),      # 50 per hour
    }

    limit, window = limits.get(action, (100, 60))

    count = await conn.fetchval("""
        SELECT COUNT(*)
        FROM share_access_log
        WHERE caller_user_id = $1
          AND action = $2
          AND created_at > NOW() - INTERVAL '1 second' * $3
    """, caller_user_id, action, window)

    if count >= limit:
        return False, f"Rate limit exceeded: {limit} per {window}s"

    return True, None
```

---

## Complexity Assessment

### Overall Complexity: **Medium → High**

This is a permissions-heavy feature that touches tools, storage, and audit logging.

| Component | Complexity | Notes |
|-----------|------------|-------|
| **Share Registry** | Low | Standard CRUD table |
| **Authorization Gate** | Medium | Centralized, but needs careful testing |
| **Read-only Tools** | Medium | Straightforward with auth gate |
| **Write Queue** | High | Worker process, LISTEN/NOTIFY, error handling |
| **Audit Logging** | Low | Append-only table |
| **Rate Limiting** | Low | Count-based on audit logs |
| **Worker Monitoring** | Medium | Health checks, alerts |

---

## Risk Assessment

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| **DuckDB write contention** | High | High | Queue serialization (mandatory) |
| **Permission bypass** | Critical | Medium | Code review; `authorize()` must be mandatory in all shared tools |
| **Worker crash → stuck jobs** | High | Medium | Startup drain; health check; auto-restart |
| **Orphaned shares** | Medium | Low | Cascade revocation trigger |
| **Resource ref drift** | Medium | Medium | Use stable IDs (UUIDs); validation function |
| **Audit gaps** | Medium | Low | Log in `authorize()`; require tools to use it |
| **Rate limit bypass** | Medium | Low | Central rate limit function; enforce before `authorize()` |
| **Queue flooding** | Medium | Low | Per-user rate limits on queue operations |
| **LISTEN/NOTIFY loss** | Medium | Low | Polling fallback; worker restart drains pending |

---

## Recommended Phasing

### Phase 0: Foundation (1-2 days)
- Create `share_registry` table and indexes
- Create `share_access_log` table
- Implement `authorize()` function with tests
- Implement `log_access()` function
- Migration script for stable IDs

**Deliverable**: Authorization infrastructure ready; no user-facing changes.

### Phase 1: Read-Only Sharing (3-5 days)
- Implement all read-only tools (8 tools)
- Share creation workflow (owner → pending → target accepts)
- Share discovery (`list_pending_shares`, `list_shares`)
- Resource access tools (`read_shared_file`, `search_shared_kb`, `query_shared_db`)
- Revoke functionality
- Integration tests for read flows

**Deliverable**: Users can share and access shared resources in read-only mode.

### Phase 2: Write Sharing (5-7 days)
- Create `shared_db_write_queue` table
- Implement write queue worker with LISTEN/NOTIFY
- Implement write tools (5 tools)
- Job status tracking (`get_write_job_status`, `list_write_jobs`)
- Worker health check endpoint
- Startup drain logic
- Integration tests for write flows

**Deliverable**: Users can write to shared resources via queue.

### Phase 3: Observability & Hardening (2-3 days)
- Rate limiting implementation
- Admin analytics dashboard (access logs, anomaly detection)
- Alerts for queue depth, worker health
- Security audit (penetration testing)
- Documentation

**Deliverable**: Production-ready sharing feature with monitoring.

---

## Implementation Checklist

- [ ] Create share registry migration
- [ ] Implement `authorize()` function
- [ ] Implement `log_access()` function
- [ ] Implement `validate_resource_ref()` function
- [ ] Create audit log migration
- [ ] Implement rate limiting
- [ ] Implement read-only tools (8)
- [ ] Implement write queue migration
- [ ] Implement write queue worker
- [ ] Implement LISTEN/NOTIFY
- [ ] Implement startup drain
- [ ] Implement worker health check
- [ ] Implement write tools (5)
- [ ] Implement job status tools (2)
- [ ] Create admin dashboard
- [ ] Write integration tests
- [ ] Security audit
- [ ] Documentation

---

## References

- Postgres LISTEN/NOTIFY: https://www.postgresql.org/docs/current/sql-notify.html
- DuckDB concurrency: https://duckdb.org/docs/sql/statements/alter_table.html
- asyncpg documentation: https://magicstack.github.io/asyncpg/
