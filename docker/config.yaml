# Executive Assistant Default Configuration
# This file contains application default settings.
# For secrets and environment-specific overrides, use .env file.

# ============================================================================
# ESSENTIAL CONFIGURATION
# ============================================================================
# Basic settings that define your agent deployment

# Agent Configuration
agent:
  # Agent display name - customize this for your deployment
  name: "Ken"

# Channels Configuration
channels:
  # Active channels (comma-separated: telegram, http)
  # Override with EXECUTIVE_ASSISTANT_CHANNELS in .env if needed
  active: telegram, http

  telegram:
    # webhook_url and webhook_secret should be in .env
    # Example:
    # TELEGRAM_WEBHOOK_URL=https://your-domain.com/webhook
    # TELEGRAM_WEBHOOK_SECRET=your-secret-here

  http:
    host: "0.0.0.0"
    port: 8000

# Admin Access Control
admin:
  user_ids: [] # Comma-separated list of admin user IDs
  thread_ids: [telegram:6282871705, http:admin] # Comma-separated list of admin thread IDs

# ============================================================================
# CORE INTELLIGENCE
# ============================================================================
# AI models, knowledge retrieval, and memory systems

# LLM Configuration
llm:
  # Default LLM provider: anthropic, openai, zhipu, deepseek, ollama, gemini, qwen, kimi, minimax
  default_provider: ollama

  # Provider-specific model configurations
  anthropic:
    default_model: claude-haiku-4-5-20251001
    fast_model: claude-haiku-4-5-20251001

  openai:
    default_model: gpt-5.2-2025-12-11
    fast_model: gpt-5-mini-2025-08-07

  zhipu:
    default_model: glm-4.7
    fast_model: glm-4.7-flash

  # DeepSeek (Official API)
  deepseek:
    default_model: deepseek-chat
    fast_model: deepseek-chat
    api_base: https://api.deepseek.com
    # Models available:
    # - deepseek-chat (V3): General purpose, excellent tool calling, fast
    # - deepseek-coder: Specialized for code generation
    # - deepseek-reasoner: Advanced reasoning, higher quality (NOT compatible with tools)

  ollama:
    default_model: deepseek-v3.1:671b-cloud
    fast_model: deepseek-v3.1:671b-cloud
    mode: cloud # cloud or local
    cloud_url: "https://ollama.com"
    local_url: "http://localhost:11434"
    # Tool Calling Compatibility (Ollama Cloud models):
    # ✅ Compatible (JSON Schema supported):
    #   - deepseek-v3.1:671b-cloud (Recommended)
    #   - glm-5:cloud, minimax-m2.1:cloud, gpt-oss:20b-cloud, qwen3-next:80b-cloud, kimi-k2.5:cloud
    # ⚠️ Special format:
    #   - deepseek-v3.2:cloud (XML <function_calls>; supported via channel compatibility parser)

  # Gemini (Google)
  gemini:
    default_model: gemini-2.5-flash
    fast_model: gemini-2.5-flash-lite
    # Optional: Vertex AI backend (defaults to Gemini API)
    vertexai: false
    project: null
    location: us-central1

  # Qwen (Alibaba)
  qwen:
    default_model: qwen-plus-latest
    fast_model: qwen-turbo-latest

  # Kimi K2 (Moonshot AI)
  kimi:
    default_model: kimi-k2.5
    fast_model: kimi-k2.5
    api_base: https://api.moonshot.ai/v1

  # MiniMax M2
  minimax:
    default_model: MiniMax-M2.1
    fast_model: MiniMax-M2-Turbo
    # Choose API type: openai or anthropic
    api_type: openai
    api_base: https://api.minimax.io/v1

# Vector Store Configuration
vector_store:
  # Sentence-transformers model for embeddings
  embedding_model: "all-MiniLM-L6-v2"
  embedding_dimension: 384
  chunk_size: 3000 # Default chunk size in characters

# Memory (Embedded User Memories)
memory:
  auto_extract: true # Auto-extract memories from each message
  confidence_min: 0.6 # Minimum confidence to save a memory
  max_per_turn: 3 # Max memories to extract per turn
  extract_model: fast # Model variant: "default", "fast", or specific model
  extract_provider: null # Override provider for extraction
  extract_temperature: 0.0

# Journal (Time-Based Activity Tracking with Rollups)
journal:
  # Rollup retention configuration (how many levels back to keep)
  # Levels: raw (0) → hourly (1) → weekly (2) → monthly (3) → yearly (4)
  retention:
    hourly: 30 # Keep hourly rollups for 30 days
    weekly: 52 # Keep weekly rollups for 52 weeks (1 year)
    monthly: 84 # Keep monthly rollups for 84 months (7 years)
    yearly: 7 # Keep yearly rollups for 7 years
  # Auto-rollup settings
  auto_rollup:
    enabled: false # Set to true to enable automatic rollup creation
    hour_trigger: "0 * * * *" # Cron expression: every hour at minute 0
    day_trigger: "0 0 * * 0" # Cron expression: every Sunday at midnight
    month_trigger: "0 0 1 * *" # Cron expression: 1st of each month at midnight

# ============================================================================
# DATA & STORAGE
# ============================================================================
# Where your data is stored and how it's organized

# Storage Configuration
storage:
  # Checkpoint storage backend: postgres, memory
  checkpoint: postgres

  # PostgreSQL connection settings
  postgres:
    host: localhost
    port: 5432
    user: ken
    db: ken_db
    # password should be in .env

  # Storage paths (relative to project root)
  paths:
    # 3-level storage hierarchy
    shared_root: "./data/shared"
    users_root: "./data/users"
    admins_root: "./data/admins"

  # Security settings
  max_file_size_mb: 10

# ============================================================================
# BEHAVIOR & FEATURES
# ============================================================================
# Advanced middleware and agent behavior settings

# LangChain Middleware Configuration
middleware:
  # Summarization Middleware - reduces context size when it gets too large
  summarization:
    enabled: true
    # [SCOPE: Total LLM request size] Includes: system prompt + tool definitions + messages
    # Triggers when last LLM request reached this count (~4,300 message tokens effective)
    max_tokens: 12000 # Trigger summarization at this token count (see note below)
    # [SCOPE: Message buffer AFTER summarization] Preserves most recent ~2,000 tokens of messages
    # Results in ~15-20 recent messages + summary (5:1 ratio prevents excessive context loss)
    target_tokens: 2000 # Target size after summarization (5:1 ratio for balanced context retention)
    # NOTE: max_tokens may trigger early (at ~4000-4500 actual message tokens) due to
    # LangChain checking usage_metadata which includes system prompt + tools overhead.
    #
    # Token accounting with ~100 tools:
    # - System prompt: ~1,200 tokens
    # - Tool definitions: ~5,000-6,000 tokens
    # - Overhead total: ~6,200-7,200 tokens
    # - Effective message trigger: 12,000 - 7,200 = ~4,800 tokens
    #
    # Ratio: 10,000 → 2,000 (5:1) provides good balance between:
    # - Preserving recent conversation (~15-20 messages)
    # - Summarizing older context
    # - Preventing context loss

  # Call Limits - prevent runaway agent execution (per-message limits)
  model_call_limit: 50 # Max LLM calls per message (prevents infinite loops)
  tool_call_limit: 100 # Max tool calls per message

  # Retry Middleware - retry failed calls
  tool_retry_enabled: true
  model_retry_enabled: true

  # Human-in-the-Loop (not yet implemented)
  hitl_enabled: false

  # Todo List Middleware
  todo_list_enabled: true

  # Context Editing Middleware
  context_editing:
    enabled: false
    trigger_tokens: 5000
    keep_tool_uses: 10

  # Status Update Middleware - real-time progress feedback
  status_updates:
    enabled: true
    show_tool_args: false # Show tool arguments (security risk if true)
    update_interval: 1 # Minimum seconds between status updates

  # Todo List Display Middleware - shows planned tasks to users
  todo_list_display:
    max_display: 10 # Maximum number of todos to show in status
    update_interval: 0.5 # Minimum seconds between todo list updates
    show_progress_bar: false # Use progress bar format instead of list

# ============================================================================
# OPTIONAL FEATURES
# ============================================================================
# Additional features that can be enabled as needed

# Logging
logging:
  level: DEBUG # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: logs/executive_assistant.log # Optional log file path

# System Configuration
system:
  # Timezone for the application (used for scheduling, timestamps, etc.)
  # Override with TZ in .env if needed for your environment
  timezone: UTC

# OCR (Text Extraction from Images/PDFs)
ocr:
  engine: surya # paddleocr or tesseract
  lang: eng
  use_gpu: false
  max_file_mb: 10
  max_pages: 100
  pdf_dpi: 200
  pdf_min_text_chars: 5
  timeout_seconds: 30
  structured_model: fast
  structured_provider: null
  structured_max_retries: 2

# Allowed File Extensions
allowed_file_extensions:
  # Documents
  - .txt
  - .md
  - .pdf
  - .csv
  - .xml
  - .json
  - .yaml
  - .yml

  # Code
  - .py
  - .js
  - .ts
  - .html
  - .css
  - .sh
  - .bash

  # Images
  - .png
  - .jpg
  - .jpeg
  - .webp
  - .tiff
  - .tif
  - .bmp
  - .gif
