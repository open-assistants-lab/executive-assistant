# Executive Assistant Default Configuration
# This file contains application default settings.
# For secrets and environment-specific overrides, use .env file.

# ============================================================================
# ESSENTIAL CONFIGURATION
# ============================================================================
# Basic settings that define your agent deployment

# Agent Configuration
agent:
  # Agent display name - customize this for your deployment
  name: "Executive Assistant"

# Channels Configuration
channels:
  telegram:
    # webhook_url and webhook_secret should be in .env
    # Example:
    # TELEGRAM_WEBHOOK_URL=https://your-domain.com/webhook
    # TELEGRAM_WEBHOOK_SECRET=your-secret-here

  http:
    host: "0.0.0.0"
    port: 8000

# Admin Access Control
admin:
  user_ids: [] # Comma-separated list of admin user IDs
  thread_ids: [telegram:6282871705] # Comma-separated list of admin thread IDs

# ============================================================================
# CORE INTELLIGENCE
# ============================================================================
# AI models, knowledge retrieval, and memory systems

# LLM Configuration
llm:
  # Default LLM provider: anthropic, openai, zhipu, ollama
  default_provider: openai

  # Provider-specific model configurations
  anthropic:
    default_model: claude-haiku-4-5-20251001
    fast_model: claude-haiku-4-5-20251001

  openai:
    default_model: gpt-5.2-2025-12-11
    fast_model: gpt-5-mini-2025-08-07

  zhipu:
    default_model: glm-4-plus
    fast_model: glm-4-flash

  ollama:
    default_model: gpt-oss:20b-cloud
    fast_model: gpt-oss:20b-cloud
    mode: cloud # cloud or local
    cloud_url: "https://ollama.com"
    local_url: "http://localhost:11434"

# Vector Store Configuration
vector_store:
  # Sentence-transformers model for embeddings
  embedding_model: "all-MiniLM-L6-v2"
  embedding_dimension: 384
  chunk_size: 3000 # Default chunk size in characters

# Memory (Embedded User Memories)
memory:
  auto_extract: true # Auto-extract memories from each message
  confidence_min: 0.6 # Minimum confidence to save a memory
  max_per_turn: 3 # Max memories to extract per turn
  extract_model: fast # Model variant: "default", "fast", or specific model
  extract_provider: null # Override provider for extraction
  extract_temperature: 0.0

# ============================================================================
# DATA & STORAGE
# ============================================================================
# Where your data is stored and how it's organized

# Storage Configuration
storage:
  # Checkpoint storage backend: postgres, memory
  checkpoint: postgres

  # PostgreSQL connection settings
  postgres:
    host: localhost
    port: 5432
    user: executive_assistant
    db: executive_assistant_db
    # password should be in .env

  # Storage paths (relative to project root)
  paths:
    # 3-level storage hierarchy
    shared_root: "./data/shared"
    users_root: "./data/users"
    admins_root: "./data/admins"

  # Security settings
  max_file_size_mb: 10

# ============================================================================
# BEHAVIOR & FEATURES
# ============================================================================
# Advanced middleware and agent behavior settings

# LangChain Middleware Configuration
middleware:
  # Summarization Middleware - reduces context size when it gets too large
  summarization:
    enabled: true
    # [SCOPE: Total LLM request size] Includes: system prompt + tool definitions + messages
    # Triggers when last LLM request reached this count (~4,300 message tokens effective)
    max_tokens: 12000 # Trigger summarization at this token count (see note below)
    # [SCOPE: Message buffer AFTER summarization] Preserves most recent ~2,000 tokens of messages
    # Results in ~15-20 recent messages + summary (5:1 ratio prevents excessive context loss)
    target_tokens: 2000 # Target size after summarization (5:1 ratio for balanced context retention)
    # NOTE: max_tokens may trigger early (at ~4000-4500 actual message tokens) due to
    # LangChain checking usage_metadata which includes system prompt + tools overhead.
    #
    # Token accounting with ~100 tools:
    # - System prompt: ~1,200 tokens
    # - Tool definitions: ~5,000-6,000 tokens
    # - Overhead total: ~6,200-7,200 tokens
    # - Effective message trigger: 12,000 - 7,200 = ~4,800 tokens
    #
    # Ratio: 10,000 â†’ 2,000 (5:1) provides good balance between:
    # - Preserving recent conversation (~15-20 messages)
    # - Summarizing older context
    # - Preventing context loss

  # Call Limits - prevent runaway agent execution (per-message limits)
  model_call_limit: 50 # Max LLM calls per message (prevents infinite loops)
  tool_call_limit: 100 # Max tool calls per message

  # Retry Middleware - retry failed calls
  tool_retry_enabled: true
  model_retry_enabled: true

  # Human-in-the-Loop (not yet implemented)
  hitl_enabled: false

  # Todo List Middleware
  todo_list_enabled: true

  # Context Editing Middleware
  context_editing:
    enabled: false
    trigger_tokens: 5000
    keep_tool_uses: 10

  # Status Update Middleware - real-time progress feedback
  status_updates:
    enabled: true
    show_tool_args: false # Show tool arguments (security risk if true)
    update_interval: 1 # Minimum seconds between status updates

  # Todo List Display Middleware - shows planned tasks to users
  todo_list_display:
    max_display: 10 # Maximum number of todos to show in status
    update_interval: 0.5 # Minimum seconds between todo list updates
    show_progress_bar: false # Use progress bar format instead of list

# ============================================================================
# OPTIONAL FEATURES
# ============================================================================
# Additional features that can be enabled as needed

# Logging
logging:
  level: DEBUG # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: logs/executive_assistant.log # Optional log file path

# OCR (Text Extraction from Images/PDFs)
ocr:
  engine: surya # paddleocr or tesseract
  lang: eng
  use_gpu: false
  max_file_mb: 10
  max_pages: 100
  pdf_dpi: 200
  pdf_min_text_chars: 5
  timeout_seconds: 30
  structured_model: fast
  structured_provider: null
  structured_max_retries: 2

# Allowed File Extensions
allowed_file_extensions:
  # Documents
  - .txt
  - .md
  - .pdf
  - .csv
  - .xml
  - .json
  - .yaml
  - .yml

  # Code
  - .py
  - .js
  - .ts
  - .html
  - .css
  - .sh
  - .bash

  # Images
  - .png
  - .jpg
  - .jpeg
  - .webp
  - .tiff
  - .tif
  - .bmp
  - .gif
